import numpy as np
import time

def compute_loss(w,features,labels):
    w_loss = 0 
    x = features
    y = labels
    total_loss = 0
    
    negative_y = y*-1
    dot_product = np.dot(x,w)
    multiplication_operation = np.multiply(dot_product, negative_y)
    exponentiate = np.exp(multiplication_operation)
    w_loss = np.log(1+exponentiate)
    total_loss = np.sum(w_loss, axis = 0)

    return total_loss

def compute_gradient(w, features, labels, learning_rate):
   
    w_gradient = np.zeros(300, float)
    
    x = features
    y = labels
    total_sum = 0
    N = len(x)
    average = 0
    
    negative_y = y*-1
    dot_product = np.dot(x,w)
    multiplication_operation = np.multiply(dot_product, negative_y)
    exponentiate = np.exp(multiplication_operation)
    fraction = exponentiate/(1+exponentiate)
    parentheses = np.multiply(fraction,negative_y)
     
    for i in range(len(x)):
        w_gradient = np.dot(parentheses, i)
    total_sum = np.sum(w_gradient, axis =0)
    average = total_sum/N
  
    new_w = w - (learning_rate*average)
    
    return new_w
   
def gradient_descent_runner(initial_w, features, labels, learning_rate, num_iterations):
    w = initial_w
    for i in range(num_iterations):
        start = time.time()
        w = compute_gradient(w, features, labels, learning_rate)
        end = time.time()
        total_time = end-start
        print("Iteration = {0} , Loss: {1}, Elapsed Time: {2} ".format(i, compute_loss(w,features,labels), total_time))
    return w

def main():  
	
    file = open("w8a.txt", "r")
    labels = []
    features = []
	
    for line in file:
        mylist = line.split(" ")
        labels.append(int(mylist[0]))
        example = [0 for i in range(300)]
		
        for i in range(1,len(mylist)-1):	
           indexAndValue = mylist[i].split(":")
           index = indexAndValue[0]
           index = int(index)
           example[index-1] = int(indexAndValue[1]) 
    
        features.append(example)
        
    labels = np.array(labels)
    features = np.array(features)
    
    initial_w = np.zeros(300,float)
    num_iterations = 1000
    learning_rate = 0.0001
    
    print("Starting Gradient Descent at loss = {}".format(compute_loss(initial_w, features,labels)))
    print("Running...")
    w = gradient_descent_runner(initial_w, features, labels, learning_rate, num_iterations)
    print("After {0} iterations, loss = {1}".format(num_iterations, compute_loss(w, features, labels)))

    
main()
